{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9deb1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0f4e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "      <td>katia abreu diz vai colocar expulsao moldura n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>ray peita bolsonaro conservador fake entrevist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fake</td>\n",
       "      <td>reinaldo azevedo desmascarado policia federal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>fake</td>\n",
       "      <td>relatorio assustador bndes mostra dinheiro pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fake</td>\n",
       "      <td>radialista americano fala sobre pt vendem ilus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index label                                  preprocessed_news\n",
       "0      0  fake  katia abreu diz vai colocar expulsao moldura n...\n",
       "1      1  fake  ray peita bolsonaro conservador fake entrevist...\n",
       "2      2  fake  reinaldo azevedo desmascarado policia federal ...\n",
       "3      3  fake  relatorio assustador bndes mostra dinheiro pub...\n",
       "4      4  fake  radialista americano fala sobre pt vendem ilus..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Carregar o arquivo CSV\n",
    "file_path = 'dataset/pre-processed.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Exibir as primeiras linhas do dataframe para entender a estrutura dos dados\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1d152",
   "metadata": {},
   "source": [
    "O `CountVectorizer` é uma técnica utilizada em processamento de linguagem natural (PLN) e aprendizado de máquina para converter uma coleção de documentos de texto em uma matriz de contagens de tokens. Essencialmente, ele transforma os textos em um formato que os algoritmos de aprendizado de máquina podem entender. Vamos detalhar suas características principais:\n",
    "\n",
    "1. **Tokenização**: Ele divide os textos em palavras, termos ou símbolos (conhecidos como tokens). Por exemplo, a frase \"O gato dorme\" seria dividida em tokens como \"O\", \"gato\", \"dorme\".\n",
    "\n",
    "2. **Construção de Vocabulário**: Cria um vocabulário de todos os tokens únicos presentes nos documentos de texto. Por exemplo, se tivermos duas frases, \"O gato dorme\" e \"O cachorro brinca\", o vocabulário seria [\"O\", \"gato\", \"dorme\", \"cachorro\", \"brinca\"].\n",
    "\n",
    "3. **Contagem de Frequência**: Para cada documento, conta quantas vezes cada palavra do vocabulário aparece. Isto resulta em uma matriz onde cada linha representa um documento e cada coluna representa um token do vocabulário. Os valores da matriz são as contagens de frequência de cada token no documento correspondente.\n",
    "\n",
    "4. **Normalização**: Opcionalmente, o CountVectorizer pode aplicar várias formas de normalização nos dados, como converter todas as letras para minúsculas para garantir que \"Gato\" e \"gato\" sejam contados como o mesmo token.\n",
    "\n",
    "O CountVectorizer é uma ferramenta básica, mas poderosa, para a conversão de texto em um formato numérico que pode ser utilizado por algoritmos de aprendizado de máquina. Ele é particularmente útil para tarefas de classificação de texto, análise de sentimento, agrupamento de documentos, entre outras aplicações em PLN. No entanto, ele tem limitações, como a incapacidade de capturar a ordem das palavras ou o contexto em que aparecem, que podem ser superadas por técnicas mais avançadas como TF-IDF ou word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ca1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo os dados em características (X) e alvo (y)\n",
    "X = data['preprocessed_news']\n",
    "y = data['label']\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criando e treinando o modelo\n",
    "# Usaremos um pipeline que inclui a vetorização e o classificador Naive Bayes\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Avaliando o modelo no conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3b2e594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8270833333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.96      0.68      0.80       718\n",
      "        true       0.76      0.97      0.85       722\n",
      "\n",
      "    accuracy                           0.83      1440\n",
      "   macro avg       0.86      0.83      0.82      1440\n",
      "weighted avg       0.86      0.83      0.82      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy);print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34594edf",
   "metadata": {},
   "source": [
    "# Sobre TfidfVectorizer\n",
    "### Para aplicar word embeddings ao modelo de classificação de texto, podemos usar o TfidfVectorizer ou incorporar embeddings pré-treinados como os do Word2Vec ou GloVe. O TfidfVectorizer é uma alternativa ao CountVectorizer que não apenas conta a frequência das palavras, mas também leva em consideração a importância relativa das palavras nos documentos. Vamos começar com o TfidfVectorizer.\n",
    "\n",
    "TfidfVectorizer: TF-IDF significa \"Term Frequency-Inverse Document Frequency\". Este método avalia a importância de uma palavra no contexto de um conjunto de documentos. Ele aumenta a pontuação de palavras que são frequentes em um documento, mas não em todos os documentos, o que ajuda a filtrar palavras comuns que não contribuem para o significado do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbd1a0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6145833333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       1.00      0.23      0.37       718\n",
      "        true       0.57      1.00      0.72       722\n",
      "\n",
      "    accuracy                           0.61      1440\n",
      "   macro avg       0.78      0.61      0.55      1440\n",
      "weighted avg       0.78      0.61      0.55      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Criando e treinando o modelo com TfidfVectorizer\n",
    "model_tfidf = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Avaliando o modelo com TfidfVectorizer no conjunto de teste\n",
    "y_pred_tfidf = model_tfidf.predict(X_test)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "report_tfidf = classification_report(y_test, y_pred_tfidf)\n",
    "\n",
    "accuracy_tfidf, report_tfidf\n",
    "print(accuracy_tfidf);print(report_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d7021",
   "metadata": {},
   "source": [
    "#### Resultado mostrou ser menor que anterior (CountVectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf315b",
   "metadata": {},
   "source": [
    "## Para explorar embeddings de palavras mais sofisticados, vamos usar GloVe com Rede Neural.\n",
    "\n",
    "Para incorporar embeddings pré-treinados como GloVe em seu modelo de classificação de texto, é necessário um processo mais complexo. Infelizmente, não posso executar esse processo diretamente aqui devido a limitações de hardware e software, mas posso fornecer uma visão geral de como você pode fazer isso em seu próprio ambiente de desenvolvimento.\n",
    "\n",
    "O processo geral envolve os seguintes passos:\n",
    "\n",
    "1. **Download do GloVe**: Primeiro, você precisa baixar os embeddings pré-treinados do GloVe. Eles estão disponíveis em várias dimensões (por exemplo, 50, 100, 200, 300).\n",
    "\n",
    "2. **Carregar os Embeddings do GloVe**: Carregar os embeddings em sua aplicação. Isso geralmente envolve ler o arquivo do GloVe e criar um dicionário onde as chaves são as palavras e os valores são os vetores de embedding correspondentes.\n",
    "\n",
    "3. **Preparação dos Dados**: Você precisa converter os textos do seu conjunto de dados em vetores usando os embeddings do GloVe. Isso geralmente significa que você precisará calcular a média dos embeddings de todas as palavras em um documento para obter um único vetor representando esse documento.\n",
    "\n",
    "4. **Construção do Modelo**: Em vez de usar modelos como Naive Bayes, você provavelmente usará um modelo de rede neural, como um perceptron multicamada, que pode processar os vetores de entrada de alta dimensão.\n",
    "\n",
    "5. **Treinamento e Avaliação**: Treine o modelo em seu conjunto de dados e avalie seu desempenho.\n",
    "\n",
    "Aqui está um exemplo de código que ilustra como isso pode ser feito em Python, utilizando bibliotecas como Pandas, NumPy e Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528b4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f6868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o GloVe embeddings\n",
    "embeddings_index = {}\n",
    "with open('glove/glove_s100.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector_values = []\n",
    "        for val in values[1:]:\n",
    "            try:\n",
    "                vector_values.append(float(val))\n",
    "            except ValueError:\n",
    "                # Pular valores que não podem ser convertidos em float\n",
    "                continue\n",
    "        if len(vector_values) == 100:  # Garantir que o vetor tenha a dimensão correta\n",
    "            embeddings_index[word] = np.asarray(vector_values, dtype='float32')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a606e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Função para converter texto em embedding\n",
    "def text_to_embedding(text, embeddings_index):\n",
    "    words = text.split()\n",
    "    embedding = np.zeros(100)  # usando GloVe 100d. Se usar de X dimensao, deve alterar\n",
    "    for word in words:\n",
    "        embedding += embeddings_index.get(word, np.zeros(100))\n",
    "    return embedding / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6336cb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.92      0.86      0.89       718\n",
      "        true       0.87      0.92      0.90       722\n",
      "\n",
      "    accuracy                           0.89      1440\n",
      "   macro avg       0.89      0.89      0.89      1440\n",
      "weighted avg       0.89      0.89      0.89      1440\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eltonss/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Carregar e preparar os dados\n",
    "file_path = 'dataset/pre-processed.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "X = data['preprocessed_news'].apply(lambda x: text_to_embedding(x, embeddings_index))\n",
    "y = data['label']\n",
    "\n",
    "# Dividir os dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Converter para NumPy arrays\n",
    "X_train = np.stack(X_train.values)\n",
    "X_test = np.stack(X_test.values)\n",
    "\n",
    "# Construir e treinar o modelo\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, ), max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179babc",
   "metadata": {},
   "source": [
    "## Como pode ser visto. Usando Glove de 100 dimensões obtemos um resultado melhor. Não fiz vários testes e nem arrumei os melhores parâmetros para rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d935cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.68034561e-02 -2.32353055e-02 -1.54998905e-01 ...  2.32099256e-01\n",
      "   8.92606739e-01  3.84067160e-02]\n",
      " [-6.23445657e-02 -9.12072500e-02 -7.44497480e-02 ...  2.59122097e-01\n",
      "   8.83955821e-01  1.14895740e-01]\n",
      " [-4.75386604e-02 -1.11356802e-02  4.51263582e-02 ...  1.55598013e-01\n",
      "   1.12777204e+00  1.14076937e-01]\n",
      " ...\n",
      " [-1.76297867e-02 -7.03959068e-05 -2.94371879e-02 ...  7.32514315e-02\n",
      "   1.22927453e+00 -6.89838079e-02]\n",
      " [-4.38665350e-02  2.07470871e-03 -3.92765862e-02 ...  4.20696719e-02\n",
      "   9.90367435e-01 -1.58652385e-02]\n",
      " [-6.35821001e-02  4.63429211e-02 -5.14707074e-04 ... -5.60717936e-02\n",
      "   9.16795302e-01 -9.95023738e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Avaliar o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "print(X_test);\n",
    "#print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bceb909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'katia abreu diz vai colocar expulsao moldura nao reclamar senadora katia abreu disse expulsao pmdb resultado acao cupula atual legenda segundo oportunista amanha vou botar moldura dourada expulsao porque maos onde veio atestado boa conduta curriculo pessoas expulsaram nao servem pais servem pais beneficios proprios disse katia abreu ue expulsao algo tao bom curriculo tanta choradeira katia sabemos motivo provavelmente katia nao valor pt partido ja deveria absorvido parece pt gostava katia somente ficasse entrincheirada dentro pmdb rebaixar demais resta katia ficar chorando pitangas todos cantos tempo ate momento pt nao cadastrou katia abreu fileiras situacao patetica agricultura dilma'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "274e29b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13571221, -0.0105681 , -0.07521114, -0.39414353, -0.10343558,\n",
       "       -0.07193613, -0.07687111, -0.23221795, -0.19007643,  0.01028797,\n",
       "       -0.15087491,  0.04944402,  0.03002042, -0.02568165, -0.03284998,\n",
       "        0.07700582,  0.05523582, -0.14133226,  0.11012743,  0.38608968,\n",
       "        0.23448052,  0.10074848,  0.08505598, -0.03315924, -0.01049605,\n",
       "        0.01111146, -0.04974558, -0.0987845 ,  0.09877653,  0.4340075 ,\n",
       "       -0.08096024,  0.13396375, -0.02433407,  0.18798847, -0.04405285,\n",
       "       -0.03214161,  0.24857459, -0.03895739, -0.3450966 ,  0.15274587,\n",
       "       -0.12499233, -0.00618638, -0.02937758, -0.14371666,  0.11727814,\n",
       "        0.04188362,  0.02630847,  0.03790322,  0.24024638, -0.20948994,\n",
       "        0.14892406,  0.10966486,  0.06385879,  0.06662013, -0.18308381,\n",
       "       -0.00645588, -0.13253987, -0.0149847 , -0.02779608, -0.06980925,\n",
       "        0.02124444,  0.13863389, -0.08555779,  0.01999292, -0.02724287,\n",
       "        0.15805948, -0.01192757,  0.137374  ,  0.11932953,  0.02101395,\n",
       "       -0.18165604, -0.01386086, -0.11992549,  0.09203513,  0.78733383,\n",
       "       -0.05456768, -0.07560495,  0.05174836,  0.01347667, -0.13297562,\n",
       "       -0.04507818, -0.64489899, -0.08296511,  0.08097892,  0.03803602,\n",
       "        0.02352664, -0.05286671, -0.11973361, -0.11048405, -0.15125762,\n",
       "       -0.05251026,  0.42399247, -0.14982821, -0.20052747, -0.03723871,\n",
       "       -0.03741498,  0.22186735,  0.16698068,  0.75556596, -0.06727877])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_embedding(data['preprocessed_news'][0], embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c2d28aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fake']\n"
     ]
    }
   ],
   "source": [
    "text = 'Vacina é perigosa porque causa hemorragia?'\n",
    "pergunta_user = text_to_embedding(text, embeddings_index)\n",
    "y_pred = model.predict(pergunta_user.reshape(-1, 100))\n",
    "print(y_pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19479861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
